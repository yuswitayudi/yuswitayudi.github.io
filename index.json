[{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/clock-is-ticking-time-management-concept-with-clock-gray-background-top-view-hands-holding-magnifier-horizontal-image_176474-6600.jpg?t=st=1737111756~exp=1737115356~hmac=643f6fb022880aab9d056bc50f1deecc4f03fd09034b8e922cde1cf9a4498a77\u0026amp;w=1380\" alt=\"timing\"\u003e\u003c/p\u003e\n\u003ch3 id=\"why-initial-delay-seconds-matter-in-kubernetes\"\u003eWhy Initial Delay Seconds Matter in Kubernetes\u003c/h3\u003e\n\u003cp\u003eHello, everyone! Long time no see huh. Today, I want to talk about a small but important feature in Kubernetes: \u003cstrong\u003einitialDelaySeconds\u003c/strong\u003e. It’s one of those settings that can save you from a lot of headaches if you understand and use it correctly.\u003c/p\u003e\n\u003ch4 id=\"what-is-initialdelayseconds\"\u003eWhat is \u003ccode\u003einitialDelaySeconds\u003c/code\u003e?\u003c/h4\u003e\n\u003cp\u003eWhen deploying an application in Kubernetes, you’ll often define liveness and readiness probes. These probes are used by Kubernetes to check if your application is healthy and ready to serve traffic.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLiveness probe\u003c/strong\u003e ensures your app is running.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReadiness probe\u003c/strong\u003e ensures your app is ready to handle requests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBut here’s the catch: your app might need some time to initialize when it starts. Maybe it’s loading configurations, setting up connections, or warming up caches. If Kubernetes starts checking the probes too early, it might think your app is unhealthy and restart it unnecessarily. That’s where \u003ccode\u003einitialDelaySeconds\u003c/code\u003e comes in.\u003c/p\u003e\n\u003ch4 id=\"why-you-should-use-it\"\u003eWhy You Should Use It\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003einitialDelaySeconds\u003c/code\u003e defines how long Kubernetes should wait before running the first probe after starting a container. Here are a few reasons why it’s crucial:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAvoid false failures:\u003c/strong\u003e\nWithout an initial delay, Kubernetes might think your app is failing simply because it’s not ready yet.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSmooth start-up process:\u003c/strong\u003e\nYou don’t want your app to get into a restart loop just because it needs a few extra seconds to initialize.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBetter user experience:\u003c/strong\u003e\nBy ensuring your app is ready before it starts serving traffic, you avoid errors for end users during the start-up phase.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"example-configuration\"\u003eExample Configuration\u003c/h4\u003e\n\u003cp\u003eHere’s an example of how to set \u003ccode\u003einitialDelaySeconds\u003c/code\u003e in your deployment YAML:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-yaml\" data-lang=\"yaml\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003elivenessProbe\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003ehttpGet\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003epath\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e/healthz\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eport\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e8080\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003einitialDelaySeconds\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eperiodSeconds\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003ereadinessProbe\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003ehttpGet\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003epath\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e/readiness\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f92672\"\u003eport\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e8080\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003einitialDelaySeconds\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e15\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003eperiodSeconds\u003c/span\u003e: \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eIn this example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003cstrong\u003eliveness probe\u003c/strong\u003e starts checking the app’s health 10 seconds after the container starts.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003ereadiness probe\u003c/strong\u003e waits 15 seconds before checking if the app is ready to serve requests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"best-practices\"\u003eBest Practices\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eUnderstand your app’s startup behavior:\u003c/strong\u003e\nTest and observe how long your app typically takes to initialize.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDon’t set arbitrary delays:\u003c/strong\u003e\nBe realistic. Setting an unnecessarily long delay could slow down the deployment process.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCombine with other probe settings:\u003c/strong\u003e\nUse \u003ccode\u003eperiodSeconds\u003c/code\u003e and \u003ccode\u003efailureThreshold\u003c/code\u003e to fine-tune how often Kubernetes checks and what defines a failure.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"conclusion\"\u003eConclusion\u003c/h4\u003e\n\u003cp\u003eUsing \u003ccode\u003einitialDelaySeconds\u003c/code\u003e might seem like a small detail, but it’s a critical one for ensuring smooth deployments and healthy applications in Kubernetes. So next time you’re configuring probes, don’t skip this setting!\u003c/p\u003e\n\u003cp\u003eThanks for reading! If you have questions or want to share your experiences, feel free to leave a comment. See you in the next post!\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/initial-delay-kubernetes/","title":"Initial Delay Kubernetes"},{"content":"\u003ch2 id=\"untuk-lebih-mengenal-saya-boleh-kunjungi-di\"\u003eSebuah manusia yang sekarang bekerja di \u003ca href=\"https://accelbyte.io/\"\u003eAccelbyte\u003c/a\u003e sebagai Site Reliability Engineer.\nUntuk lebih mengenal saya boleh kunjungi di\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/yuswitayudi\"\u003eGithub\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://instagram.com/yuswitayudi\"\u003eInstagram\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.linkedin.com/in/yudi-yuswita-sunarto-024301115/\"\u003eLinkedIn\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"thanks-slur\"\u003eThanks slur\u003c/h2\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/about/","title":"About"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/networking-concept-still-life-assortment_23-2149035669.jpg?w=900\u0026amp;t=st=1673616478~exp=1673617078~hmac=27e52e4009e468080a51d0c333ab5da2d7593782072ee7452b767563975b7b23\" alt=\"awskubernetes\"\u003e\u003c/p\u003e\n\u003cp\u003eBy the way this is the new journey to handled cloud provider like AWS, with complexity because I don\u0026rsquo;t have many knowledge about that. Okay back to the topic.\u003c/p\u003e\n\u003cp\u003eRequirement you must have:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAWS CLI\u003c/li\u003e\n\u003cli\u003ekubectl\u003c/li\u003e\n\u003cli\u003eaccess key and secret key AWS\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the first make sure you can access the AWS from aws CLI, run this command\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eaws eks list-cluster\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eif you dont have any default region, you can add with \u003ccode\u003e--region\u003c/code\u003e parameter. Ex: \u003ccode\u003e--region us-west-1\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAfter that you will show what the k8s cluster name, for access the cluster name who show up after your list eks. Just run this command to apply in kubeconfig\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eaws eks --region region_name update-kubeconfig --name cluster_name\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eand check with \u003ccode\u003ekubectl get pod\u003c/code\u003e for ensure that kubectl can access eks cluster.\u003c/p\u003e\n\u003cp\u003eadditional:\u003c/p\u003e\n\u003cp\u003ewhen you save aws acces/secret key on file. you must activate with profile name, for the example\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e aws eks --region region_name update-kubeconfig --name cluster_name --profile profile_name\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eJust like that for notes todah, hopefully I can still exist to update this \u003ca href=\"/\"\u003ewebsite\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/aws-kubeconfig/","title":"How to connect AWS eks from kubectl"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/hands-holding-puzzle-business-problem-solving-concept_53876-129016.jpg?w=826\u0026amp;t=st=1671834901~exp=1671835501~hmac=d3b2f497fb6ee4ba2aa335e08579bc27d600c92e383e68b5fa61fd4bd7a368b4\" alt=\"botkube\"\u003e\u003c/p\u003e\n\u003cp\u003eBotkube is one of the best tool opensource I know, this very simple to install and useful. The mechanism is Botkube need token to connect each other with slack. So the first thing we need to install app on slack to configure collaborate with botkube. Actually documentation of botkube is helpull and complete, but you know this is not tutorial page :)). I just want to keep in my \u003ca href=\"/posts\"\u003enotes\u003c/a\u003e if in  anytime I need.\u003c/p\u003e\n\u003ch3 id=\"first--install-app-on-slack\"\u003eFirst | Install app on slack\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eGo to \u003ca href=\"https://api.slack.com/apps\"\u003eSlack App Console\u003c/a\u003e to create an application.\u003c/li\u003e\n\u003cli\u003eClick Create New App and select From an app manifest in the popup to create application from manifest.\u003c/li\u003e\n\u003cli\u003eSelect a workspace where you want to create application and click Next.\u003c/li\u003e\n\u003cli\u003eSelect YAML tab, copy \u0026amp; paste one of the following manifests, and click Next, and then Create.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePublic Channel only\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edisplay_information:\n  name: Botkube\n  description: Botkube\n  background_color: \u0026#34;#a653a6\u0026#34;\nfeatures:\n  bot_user:\n    display_name: Botkube\n    always_online: false\noauth_config:\n  scopes:\n    bot:\n      - channels:read\n      - app_mentions:read\n      - chat:write\n      - files:write\nsettings:\n  event_subscriptions:\n    bot_events:\n      - app_mention\n  interactivity:\n    is_enabled: true\n  org_deploy_enabled: false\n  socket_mode_enabled: true\n  token_rotation_enabled: false\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePrivate Channel Only\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edisplay_information:\n  name: Botkube\n  description: Botkube\n  background_color: \u0026#34;#a653a6\u0026#34;\nfeatures:\n  bot_user:\n    display_name: Botkube\n    always_online: false\noauth_config:\n  scopes:\n    bot:\n      - groups:read\n      - app_mentions:read\n      - chat:write\n      - files:write\nsettings:\n  event_subscriptions:\n    bot_events:\n      - app_mention\n  interactivity:\n    is_enabled: true\n  org_deploy_enabled: false\n  socket_mode_enabled: true\n  token_rotation_enabled: false\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"second--install-botkube-to-the-slack-workspace\"\u003eSecond | Install Botkube to the Slack workspace\u003c/h3\u003e\n\u003cp\u003eOnce the application is created, you will be redirected to application details page. Press the \u003cstrong\u003eInstall your app\u003c/strong\u003e button, select the workspace and click \u003cstrong\u003eAllow to finish installation\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"third--obtain-bot-token\"\u003eThird | Obtain Bot Token\u003c/h3\u003e\n\u003cp\u003eFollow the steps to obtain the Bot Token:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eSelect OAuth \u0026amp; Permissions section on the left sidebar. On this page you can copy the bot token which starts with xoxb\u0026hellip;.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExport Slack Bot Token as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e export SLACK_API_BOT_TOKEN=\u0026quot;{botToken}\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"fourth--generate-and-obtain-app-level-token\"\u003eFourth | Generate and obtain App-Level Token\u003c/h3\u003e\n\u003cp\u003eSlack App with Socket Mode requires an App-Level Token for the websocket connection.\nFollow the steps to generate an App-Level Token:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eSelect Basic Information link from the left sidebar and scroll down to section App-Level Token. Click on the Generate Token and Scopes button.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEnter a Name, select connections:write scope, and click Generate.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopy App-Level Token and export it as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e export SLACK_API_APP_TOKEN=\u0026quot;${appToken}\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"five--add-botkube-user-to-a-slack-channel\"\u003eFive | Add Botkube user to a Slack channel\u003c/h3\u003e\n\u003cp\u003eAfter installing Botkube app to your Slack workspace, you could see a new bot user with the name \u0026ldquo;Botkube\u0026rdquo; added in your workspace. Add that bot to a Slack channel you want to receive notification in. You can add it by inviting @Botkube in a channel.\u003c/p\u003e\n\u003ch2 id=\"kubernetes\"\u003eKubernetes\u003c/h2\u003e\n\u003ch3 id=\"install-botkube-backend-in-kubernetes-cluster\"\u003eInstall Botkube Backend in Kubernetes cluster\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe use Helm to install Botkube in Kubernetes. Follow this guide to install helm if you don\u0026rsquo;t have it installed already.\u003c/li\u003e\n\u003cli\u003eAdd botkube chart repository:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExecute this command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    helm repo add botkube https://charts.botkube.io\n    helm repo update\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eDeploy Botkube backend using helm install in your cluster:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExecute this command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    export CLUSTER_NAME={cluster_name}\n    export ALLOW_KUBECTL={allow_kubectl}\n    export SLACK_CHANNEL_NAME={channel_name}\n\n    helm install --version v0.16.0 botkube --namespace botkube --create-namespace \\\n    --set communications.default-group.socketSlack.enabled=true \\\n    --set communications.default-group.socketSlack.channels.default.name=${SLACK_CHANNEL_NAME} \\\n    --set communications.default-group.socketSlack.appToken=${SLACK_API_APP_TOKEN} \\\n    --set communications.default-group.socketSlack.botToken=${SLACK_API_BOT_TOKEN} \\\n    --set settings.clusterName=${CLUSTER_NAME} \\\n    --set executors.kubectl-read-only.kubectl.enabled=${ALLOW_KUBECTL} \\\n    botkube/botkube`3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e- SLACK_CHANNEL_NAME is the channel name where @Botkube is added\n- SLACK_API_BOT_TOKEN is the Token you received after installing Botkube app to your Slack workspace\n- SLACK_API_APP_TOKEN is the Token you received after installing Botkube app to your Slack workspace and generate in - App-Level Token section\n- CLUSTER_NAME is the cluster name set in the incoming messages\n- ALLOW_KUBECTL set true to allow kubectl command execution by Botkube on the cluster\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSend @Botkube ping in the channel to see if Botkube is running and responding.\u003c/p\u003e\n\u003cp\u003eWith the default configuration, Botkube will watch all the resources in all the namespaces for create, delete and error events.\u003c/p\u003e\n\u003cp\u003eIf you wish to monitor only specific resources, follow the steps given below:\u003c/p\u003e\n\u003cp\u003eCreate a new config.yaml file and add Kubernetes resource configuration as described on the source page.\u003c/p\u003e\n\u003cp\u003ePass the YAML file as a flag to helm install command, e.g.:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehelm install --version v0.16.0 --name botkube --namespace botkube --create-namespace -f /path/to/config.yaml --set=...other args..\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://docs.botkube.io/installation/slack/\"\u003ebig source\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/botkube-and-slack/","title":"Botkube and Slack to Monitor and Notification"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/infrastructure-bridge_1127-2226.jpg?w=1380\u0026amp;t=st=1669198797~exp=1669199397~hmac=f55619eceb4191fb043a3ca8fe30cd9d2bbffc8f296b1924eaa5e4131502e7aa\" alt=\"multiple way\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is maybe one of the longest notes in this notes. Here we go\u0026hellip;.\u003c/p\u003e\n\u003cp\u003eFirst before I meet this ansible roles, when I want to deploy domain name with nginx server. I only install and deploy ssl to manage HTTPS certbot with manually installation which is use standard command line. This is very repetitive and there will be chances human error.\u003c/p\u003e\n\u003cp\u003erequirements:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enginx\u003c/li\u003e\n\u003cli\u003eansible-playbook\u003c/li\u003e\n\u003cli\u003essh connection\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo when I feel I can be better than it, I decide to use ansible to create repetitive task. And after the long way I create this ansible roles:\u003c/p\u003e\n\u003ch3 id=\"step-1\"\u003eStep 1\u003c/h3\u003e\n\u003cp\u003eI assume that you have knowledge about ansible playbook, basically ansible playbook need roles and host to apply.\u003c/p\u003e\n\u003cp\u003efirst you must create \u003ccode\u003emain.yml\u003c/code\u003e on roles folder.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e- name: Install certbot\n  package: \n    update_cache: yes \n    name: \n      - certbot\n      - python3-certbot-nginx\n    state: latest\n\n- name: Copy file config nginx-domain.conf to target directory\n  template:\n    src: files/nginx-domain.conf\n    dest: \u0026#34;/etc/nginx/sites-available/{{ nginx_domain_name }}\u0026#34;\n\n- name: Check folder exist\n  stat: path=/etc/nginx/sites-enabled/{{ nginx_domain_name }}\n  register: stat_result\n\n- name: Unlinking file config if exist sites-enabled\n  when: stat_result.stat.exists\n  shell:\n     cmd: unlink /etc/nginx/sites-enabled/{{ nginx_domain_name }}\n\n- name: Linking file config nginx config into sites-enabled\n  shell:\n     cmd: ln -s /etc/nginx/sites-available/{{ nginx_domain_name }} /etc/nginx/sites-enabled/\n\n- name: restart nginx\n  shell:\n     cmd: service nginx restart\n\n- name: Generate new certificate if one doesn\u0026#39;t exist.\n  shell: \u0026#34;certbot --nginx --noninteractive --agree-tos --email {{ certbot_email }} -d {{ nginx_domain_name }}\u0026#34;\n\n- name: restart nginx\n  shell:\n     cmd: service nginx reload\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eDescription:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ein the \u003ccode\u003emain.yml\u003c/code\u003e above run to install certbot and then copy template \u003ccode\u003enginx-domain.conf\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ecopy \u003ccode\u003enginx-domain.conf\u003c/code\u003e to config nginx directory\u003c/li\u003e\n\u003cli\u003egenerate new certificate whit \u003ccode\u003ecertbot\u003c/code\u003e command\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"nginx-domainconf-template\"\u003enginx-domain.conf template\u003c/h4\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eserver {\n   server_name {{ nginx_domain_name }};\n   access_log /var/log/nginx/{{ nginx_domain_name }}-acc.log;\n   error_log /var/log/nginx/{{ nginx_domain_name }}-err.log;\n   listen 80;\n   root {{ nginx_root_path }};\n\n   location / {\n \t\ttry_files $uri $uri/ =404;\n    }\n\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eI put variable \u003ccode\u003enginx_domain_name\u003c/code\u003e and \u003ccode\u003enginx_root_path\u003c/code\u003e on folder \u003ccode\u003egroup_vars\u003c/code\u003e with \u003ccode\u003edomain-nginx\u003c/code\u003e filename.\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/ansible-create-https/","title":"Create SSL Nginx with Ansible"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/two-confident-business-man-shaking-hands-during-meeting-office-success-dealing-greeting-partner-concept_1423-199.jpg?w=740\u0026amp;t=st=1670293052~exp=1670293652~hmac=cd72d25bdb2837201c78d9b5e02860bb99c48137108114a9821c62e48fe96b27\" alt=\"connections\"\u003e\u003c/p\u003e\n\u003cp\u003eJenkins merupakan salahsatu aplikasi CI/CD yang sering digunakan untuk deployment sebuah aplikasi di production. Karena kemudahan untuk customizable deployment bisa dengan \u003cem\u003edeclarative pipeline\u003c/em\u003e maupun \u003cem\u003efreestyle pipeline\u003c/em\u003e. Oleh karena kemudahan dan manfaat yang besar tak khayal jenkins menjadi salah satu tool \u003ca href=\"/tags/ci/cd\"\u003eCI/CD\u003c/a\u003e favorit. (gilak sih mirip reporter)\u003c/p\u003e\n\u003cp\u003eDalam perkembangannya ketika kita menggunakan Jenkins untuk bisa mengetahui seluruh pipeline sudah selesai berjalan atau belum terkadang kita harus mengakses dashboard untuk mengetahui prosesnya. Sebenarnya ada plugin sendiri dari jenkins menyediakan notifikasi ke email ketika pipeline gagal selesai, namun tidak ada notifikasi kapan pipeline jalan dan selesai.\u003c/p\u003e\n\u003cp\u003eOleh karena itu perlu menggunakan tool lain yakni kolaborasi Slack x Jenkins. Berikut stepnya:\u003c/p\u003e\n\u003ch4 id=\"requirements\"\u003eRequirements\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eAkun \u003ca href=\"https://slack.com\"\u003eslack\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eJenkins\u003c/li\u003e\n\u003cli\u003eJenkins CI (aplikasi dari slack untuk koneksi ke jenkins)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"tambahkan-jenkins-ci-kedalam-workspace\"\u003eTambahkan Jenkins CI kedalam workspace\u003c/h4\u003e\n\u003cp\u003eKunjungi slack app directory. Cari Jenkins CI dan install ke workspace yang akan kita gunakan untuk notifikasi monitoring.\n\u003ca href=\"https://slack.com/apps/A0F7VRFKN-jenkins-ci?tab=more_info\"\u003ehttps://slack.com/apps/A0F7VRFKN-jenkins-ci?tab=more_info\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"install-plugin-slack-notification\"\u003eInstall Plugin Slack Notification\u003c/h4\u003e\n\u003cp\u003ePada jenkins memerlukan plugin agar bisa terhubung ke API slack, dengan menginstall plugin \u003ccode\u003eSlack Notifiactions\u003c/code\u003e\n\u003cimg src=\"https://a.slack-edge.com/80588/img/integrations/jenkins-ci_step2.png\" alt=\"slack\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSetelah terinstall tambahan credentials menggunakan type \u003ccode\u003eSecrets text\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eMasuk ke manage jenkins -\u0026gt; Configure System dan cari pengaturan tentang \u003ccode\u003eSlack\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTambahkan workspace sesuai nama yang sudah dibuat ketika register slack\u003c/li\u003e\n\u003cli\u003eTambahkan credentials sesuai yang sudah dibuat tadi\u003c/li\u003e\n\u003cli\u003ePilih channel yang akan digunakan untuk monitoring berawal dengan #\u003c/li\u003e\n\u003cli\u003eTest connection apakah sudah bisa terhubung atau belum\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"menambahkanpost-build-actions\"\u003eMenambahkanPost-build Actions\u003c/h4\u003e\n\u003cp\u003eSetelah koneksi berhasil dilakaukan, selanjutnya untuk semua pipeline kita perlu menambahkan Post-build Actions dan memilih \u003ccode\u003eSlack Notifications\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://a.slack-edge.com/80588/img/integrations/jenkins-ci_step5.png\" alt=\"slack 1\"\u003e\n\u003cimg src=\"https://a.slack-edge.com/80588/img/integrations/jenkins-ci_step6.png\" alt=\"slack 2\"\u003e\u003c/p\u003e\n\u003cp\u003eDan setiap proses ketika pipeline jalan dan selesai akan ternotifikasi ke slack.\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/jenkins-ci-slack/","title":"Menghubungkan Slack dan Jenkins untuk mendapatkan notifikasi"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/vintage-tailor-ruler-isolated-down-part-aged-wooden-table_346278-918.jpg?w=1380\u0026amp;t=st=1669281043~exp=1669281643~hmac=cf874a267123fad2f648bda388d90fb2f41d1940869263dc9bcdeb2e78c70e6f\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eUkuran log yang besar pasti suatu saat akan membuatmu mikir-mikir tentang disk kan. Nah untuk itu kita perlu mengatur berapa ukuran yang ada pada docker container. Ini dijalankan dengan docker-compose maupun docker container.\u003c/p\u003e\n\u003ch3 id=\"docker-compose\"\u003eDocker compose\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eversion: \u0026#39;3\u0026#39;\nservices:\n  nginx:\n    image: nginx:latest\n    hostname: ubuntu\n    logging:\n     options:\n       tag: \u0026#34;{{.Name}}\u0026#34;\n       max-size: 50m\n    labels:\n      - \u0026#34;autoheal=true\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"docker-container\"\u003eDocker container\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003edocker run -d --name nginx --log-opt max-size=50m\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eDari kedua cara diatas akan membatasi log ke ukuran 50 megabytes saja. Oiya tambahan di file docker-compose diatas terdapat label untuk autoheal, itu digunakan untuk membuat container baru ketika container statusnya sudah \u003ccode\u003eunhealthy\u003c/code\u003e. Sumber bisa baca disini \u003ca href=\"https://hub.docker.com/r/willfarrell/autoheal/\"\u003ehttps://hub.docker.com/r/willfarrell/autoheal/\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/mengganti-log-size/","title":"Mengganti Ukuran Log Docker Container"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/construction-works-frankfurt-downtown-germany_1268-20907.jpg?w=1480\u0026amp;t=st=1669106538~exp=1669107138~hmac=b90b84671fbb8676c7985802ca45dd89f031809cefcb4be0b4adedf5433fd964\" alt=\"Trigger folder\"\u003e\u003c/p\u003e\n\u003cp\u003eYakkk\u003c/p\u003e\n\u003cp\u003eAgain-again semua catatan yang telah tertulis di web ini murni dari sudut pandang sang penulis ya, jadi apabila ada perbedaan ya wajar dong.\u003c/p\u003e\n\u003cp\u003eOke langsung ke contoh kasus, biasanya untuk depoyment menggunakan metode gitfow akan ada proses trigering untuk melakukan suatu aksi ketika sudah merged ke salah satu branch. Contoh apabila ada merged ke branch staging, maka akan triggering ke webhook yang ada di jenkins untuk deployment pada service yang berada pada environment staging. Begitupun jika ada merged pada branch production akan melakukan hal yang sama.\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eDev\u003c/th\u003e\n          \u003cth\u003eStaging\u003c/th\u003e\n          \u003cth\u003eProduction\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNamun bagaimana jika dalam suatu repo yang terdapat pada gitlab memilik banyak service? dan kita hanya ingin melakukan deployment pada service yang berada pada folder tertentu saja? Inilah jawabannya :))\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eProject_repository\n|____Frontend\n|    |___Readme.md\n|        |___app/\n|____Backend\n|    |___Readme.md\n|        |___app/\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNah semisal kita memiliki contoh struktur folder seperti diatas, semisal suatu aplikasi dijalankan melalui folder \u003ccode\u003eFrontend\u003c/code\u003e. Kita hanya akan melakukan deployment ketika ada perubahan pada \u003ccode\u003eFrontend\u003c/code\u003e saja. Maka yang perlu dilakukan pada config \u003ccode\u003e.gitlab-ci.yml\u003c/code\u003e adalah sebagai berikut\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ebefore_script:\n  - your_command_before_run_script\n\nstages:\n  - deploy\n\ndeploy_frontend:\n  stage: deploy\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \u0026#34;push\u0026#34; \u0026amp;\u0026amp; $CI_COMMIT_BRANCH == \u0026#34;staging\u0026#34;\n      changes:\n        - frontend/**/*\n  script: \n    - your_command_to_deploy_app ex: curl jenkins url webhook\n\u003c/code\u003e\u003c/pre\u003e\u003ch4 id=\"keteragan\"\u003eKeteragan:\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003epada rules diatas menggunakan \u003ccode\u003e$CI_PIPELINE_SOURCE == \u0026quot;push\u0026quot; \u0026amp;\u0026amp; $CI_COMMIT_BRANCH == \u0026quot;staging\u0026quot;\u003c/code\u003e itu berarti ketika ada mode \u003ccode\u003epush\u003c/code\u003e pada branch \u003ccode\u003estaging\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003edan ketika ada perubahan pada folder \u003ccode\u003efrontend\u003c/code\u003e maka gitlab ci akan menjalankan \u003ccode\u003escript\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eJadi tinggal sesuaikan dengan kebutuhan pada environment masing-masing.\u003c/p\u003e\n\u003cp\u003eGitu dulu lah yaaa\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/gitlab-ci-perubahan-folder/","title":"Melakukan Deploy ketika ada Perubahan pada Folder Menggunakan GitLab CI"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/confident-company-manager-giving-working-tasks-diverse-teamworkers-analysing-paperwork-with-graphs-sitting-start-up-office_482257-5103.jpg?w=1380\u0026amp;t=st=1668658940~exp=1668659540~hmac=dfa40a521887a9fc24159d1b62b4dc1170389a4cc1c3d6e237c23f489dfcdf0b\" alt=\"New task\"\u003e\u003c/p\u003e\n\u003cp\u003eNahh\u003c/p\u003e\n\u003cp\u003eKali ini saya menulis tentang \u003ca href=\"https://skaffold.dev/\"\u003eskaffold\u003c/a\u003e, merupakan software development tool untuk membuat kinerja para developer yang melakukan repetitif menjadi mudah. Kenapa demikian? mari kita telisik dan telusuk.\u003c/p\u003e\n\u003cp\u003ePada umumnya seorang developer ketika mengerjakan suatu aplikasi, dia akan melakukan dan menjalankan di devicenya sendiri. Meskipun sudah menggunakan metode docker/container namun kadang masih terdapat perbedaan ketika sudah terdeploy pada server. Atau semisal ketika menjalankan sudah melakukan development pada kubernetes, pasti ada proses build image dan push ke registry dan hal tersebut sangat repetitif.\nPada kasus kali ini catatanya adalah:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBagaimana developer debugging dengan mudah\u003c/li\u003e\n\u003cli\u003eMenghindari repetitif task untuk build docker images\u003c/li\u003e\n\u003cli\u003etidak memakan banyak resource yang berada pada server development maupun devicenya sendiri\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"install-skaffold\"\u003eInstall skaffold\u003c/h3\u003e\n\u003cp\u003ebisa melalui web langsung di \u003ca href=\"https://skaffold.dev/docs/install/#standalone-binary\"\u003esini\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"tambahkan-manifest\"\u003eTambahkan manifest\u003c/h3\u003e\n\u003cp\u003eletakkan manifest kedalam direktori repository kita yang sudah terbuat dan masukkan ke dalam folder. Contoh di folder manifest\u003c/p\u003e\n\u003ch3 id=\"lakukan-inisiasi\"\u003eLakukan inisiasi\u003c/h3\u003e\n\u003cp\u003ejalankan \u003ccode\u003eskaffold init\u003c/code\u003e untuk mendeteksi Dockerfile maupun komponen yang akan dibuild, dan juga mendeteksi manfest yang akan diterapkan\u003c/p\u003e\n\u003ch3 id=\"deploy-di-kubernetes\"\u003eDeploy di kubernetes\u003c/h3\u003e\n\u003cp\u003ejalankan \u003ccode\u003eskaffold dev\u003c/code\u003e untuk menjalankan aplikasi pada kubernetes, pada proses tersebut akan build, push image dan memasang pada cluster tujuan.\u003c/p\u003e\n\u003cp\u003eContoh sederhanan pengaplikasian skaffold ada pada repo berikut \u003ca href=\"https://github.com/yuswitayudi/simple-skaffold-nginx.git\"\u003ehttps://github.com/yuswitayudi/simple-skaffold-nginx.git\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/skaffold-simple/","title":"Menggunakan Skaffold untuk Kemudahan Development"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/premium-photo/bricklayer-industrial-worker-installing-brick-masonry-with-trowel-putty-knife-construction-site_33835-1135.jpg?w=1380\" alt=\"terraform\"\u003e\u003c/p\u003e\n\u003cp\u003eYaaa memang bukan suatu hal yang wah, sekali lagi diriku mengingatkan bahwa web ini hanya sekedar catatan biasa oleh \u003ca href=\"http://github.com/yuswitayudi\"\u003eYudi\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eSebenarnya membuat droplet pada DigitalOcean bisa sekali menggunakan Web UI melalui browser, akan tetapi jika suatu saat kita akan membuat droplet dengan jumlah yang banyak atau bisa dibilang repetitif dan dengan spesifikasi droplet yang sama. Maka butuh banyak klak klik klak klik oleh jari kita untuk bisa mencapai tujuan tersebut.\u003c/p\u003e\n\u003cp\u003eOleh karena itu, berbekal ilmu saya yang masih sedikit di \u003ca href=\"https://yuswitayudi.github.io\"\u003ecatatan\u003c/a\u003e saya menambah untuk bagaimana cara membuat droplet menggunakan terraform.\u003c/p\u003e\n\u003ch2 id=\"main\"\u003eMain\u003c/h2\u003e\n\u003cp\u003eUntuk sekarang saya belum menuliskan cara menggunakan terraform ya. Jadi saya berharap yang membaca tahu dasarnya terlebih dahulu soal terraform, mungkin next catatan boleh lah dibuat basic tentang terraform.\u003c/p\u003e\n\u003cp\u003eTerraform sendiri membutuhkan file \u003ccode\u003emain.tf\u003c/code\u003e yang mendefinisikan resources dan hal lain sesuai kebutuhan, dan berikut conton \u003ccode\u003emain.tf\u003c/code\u003e untuk membuat droplet pada DigitalOcean.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eterraform {\n  required_version = \u0026#34;\u0026gt;= 1.0.0\u0026#34;\n\n  required_providers {\n    digitalocean = {\n      source  = \u0026#34;digitalocean/digitalocean\u0026#34;\n      version = \u0026#34;~\u0026gt; 2.0\u0026#34;\n    }\n  }\n}\n\nprovider \u0026#34;digitalocean\u0026#34; {}\n\nresource \u0026#34;digitalocean_droplet\u0026#34; \u0026#34;terramino\u0026#34; {\n  image     = \u0026#34;ubuntu-22-10-x64\u0026#34;\n  name      = \u0026#34;Your-Hostname-Droplet\u0026#34;\n  region    = \u0026#34;sgp1\u0026#34;\n  size      = \u0026#34;s-2vcpu-4gb\u0026#34;\n  user_data = file(\u0026#34;terramino_app.yaml\u0026#34;)\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003enah dari file diatas ada satu file bernama \u003ccode\u003eterramino_app.yaml\u003c/code\u003e di file tersebut kita bisa mendefinisikan apa yang akan dilakukan setelah droplet dibuat. Contoh.\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#cloud-config\ngroups:\n  - ubuntu: [root,sys]\n  - your_username\n\n# Add users to the system. Users are added after groups are added.\nusers:\n  - default\n  - name: your_username\n    gecos: your_username\n    shell: /bin/bash\n    primary_group: your_username\n    sudo: ALL=(ALL:ALL) ALL\n    groups: users, admin\n    lock_passwd: false\n    ssh_authorized_keys:\n      - ssh pub key\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e2 File tersebut adalah basic dari pembuatan droplet, jalankan \u003ccode\u003eterraform init\u003c/code\u003e untuk inisiasi dan \u003ccode\u003eterraform apply\u003c/code\u003e untuk membuat droplet pada digitialocean.\u003c/p\u003e\n\u003cp\u003eOh iya pada terraform ini kita membutuhkan API_TOKEN dari DigitalOcean untuk bisa berkomunikasi. Genarate token terlebih dahulu pada web DigitalOcean pada menu API dan jalankan perintah \u003ccode\u003eexport DIGITALOCEAN_ACCESS_TOKEN=your_token\u003c/code\u003e. Sebenarnya untuk API_TOKEN bisa ditambahkan pada credentials di file, namun untuk kali ini kita menggunakan cara mendefinisikan pada environment variable.\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/terraform-digitalocean/","title":"Membuat Droplet pada DigitalOcean menggunakan Terraform"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/walls-apartment-is-construction-remodeling-renovation-extension-restoration-reconstruction_493343-29444.jpg?w=1380\u0026amp;t=st=1668069749~exp=1668070349~hmac=554ede8dc03392689097aa21dd2e62624e30956a0bd2b8a771fbbd6733eef658\" alt=\"openvpn\"\u003e\u003c/p\u003e\n\u003cp\u003eIn another case we already setup openvpn with docker, and we want to change IP Address of vpn network. How we do it? But if you interest how to setup openvpn you just only visit this \u003ca href=\"https://github.com/kylemanna/docker-openvpn\"\u003egithub repo\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIn my case I already setup IP on 192.168.x.x but at one time we use that IP on wifi/router local. So that IP not work to access. To generate new IP we must generate new \u003cstrong\u003eopenvpn.conf\u003c/strong\u003e and specify the IP Address\u003c/p\u003e\n\u003cp\u003eJust run this command:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003edocker run -v $OVPN_DATA:/etc/openvpn --rm kylemanna/openvpn ovpn_genconfig -u udp://VPN.SERVERNAME.COM -s 10.10.0.0/24\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAfter this you must restart docker container to apply new openvpn.conf.\u003c/p\u003e\n\u003cp\u003eTaraa\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/change-subnet/","title":"Change Subnet IP on OpenVPN Docker"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/premium-photo/https-encryption-improve-security-https-concept-with-search-sign-checkmark_507676-606.jpg?w=1800\" alt=\"SSL\"\u003e\u003c/p\u003e\n\u003cp\u003eIn this \u003ca href=\"https://yuswitayudi.github.io\"\u003enotes\u003c/a\u003e I will write on English, I just learn and get used to English.\u003c/p\u003e\n\u003cp\u003eFirst thing first this my \u003ca href=\"https://yuswitayudi.github.io\"\u003enotes\u003c/a\u003e with \u003cstrong\u003eCloudflare\u003c/strong\u003e domain management.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start, I have problem to generate ssl on local network. Because local network don\u0026rsquo;t have public IP to be called from certbot, to authenticate that domain is valid.\u003c/p\u003e\n\u003cp\u003eSo from \u003ca href=\"https://linggar.asia/\"\u003emy boss\u003c/a\u003e I have knowledge to solve the problem with generate certificate from certbot with authenticate challange with DNS.\u003c/p\u003e\n\u003ch2 id=\"the-command-is-particulary-like-this\"\u003eThe command is particulary like this\u003c/h2\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo certbot certonly --manual --preferred-challenges=dns --manual-auth-hook /path_to_execute/authenticator.sh --manual-cleanup-hook /path_to_execute/cleanup.sh -d domain_name\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eDescription:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ecertonly\u003c/strong\u003e : Obtain or renew a certificate, but do not install it\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u0026ndash;prefered-challenges\u003c/strong\u003e : A sorted, comma delimited list of the preferred challenge to use during authorization with the most preferred challenge listed first (Eg, \u0026ldquo;dns\u0026rdquo; or \u0026ldquo;http,dns\u0026rdquo;).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u0026ndash;manual-auth-hook\u003c/strong\u003e : script will be run before generate ssl\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u0026ndash;manual-cleanup-hook\u003c/strong\u003e : script will be run after generate ssl\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e-d\u003c/strong\u003e : domain name which generate certificates\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBased on above command we know that exist script to execute, and here it is\u003c/p\u003e\n\u003cp\u003eto generate dns with \u003ccode\u003eauthenticator.sh\u003c/code\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/bash\n\n# Get your API key from https://www.cloudflare.com/a/account/my-account\nAPI_KEY=\u0026#34;api_token_cloudflare\u0026#34;\nEMAIL=\u0026#34;your_email\u0026#34;\n\n# Strip only the top domain to get the zone id\nDOMAIN=$(expr match \u0026#34;$CERTBOT_DOMAIN\u0026#34; \u0026#39;.*\\.\\(.*\\..*\\)\u0026#39;)\n\n# Get the Cloudflare zone id\nZONE_EXTRA_PARAMS=\u0026#34;status=active\u0026amp;page=1\u0026amp;per_page=20\u0026amp;order=status\u0026amp;direction=desc\u0026amp;match=all\u0026#34;\nZONE_ID=$(curl -s -X GET \u0026#34;https://api.cloudflare.com/client/v4/zones?name=$DOMAIN\u0026amp;$ZONE_EXTRA_PARAMS\u0026#34; \\\n     -H     \u0026#34;X-Auth-Email: $EMAIL\u0026#34; \\\n     -H     \u0026#34;X-Auth-Key: $API_KEY\u0026#34; \\\n     -H     \u0026#34;Content-Type: application/json\u0026#34; | python -c \u0026#34;import sys,json;print(json.load(sys.stdin)[\u0026#39;result\u0026#39;][0][\u0026#39;id\u0026#39;])\u0026#34;)\n\n# Create TXT record\nCREATE_DOMAIN=\u0026#34;_acme-challenge.$CERTBOT_DOMAIN\u0026#34;\nRECORD_ID=$(curl -s -X POST \u0026#34;https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records\u0026#34; \\\n     -H     \u0026#34;X-Auth-Email: $EMAIL\u0026#34; \\\n     -H     \u0026#34;X-Auth-Key: $API_KEY\u0026#34; \\\n     -H     \u0026#34;Content-Type: application/json\u0026#34; \\\n     --data \u0026#39;{\u0026#34;type\u0026#34;:\u0026#34;TXT\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;\u0026#39;\u0026#34;$CREATE_DOMAIN\u0026#34;\u0026#39;\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;\u0026#39;\u0026#34;$CERTBOT_VALIDATION\u0026#34;\u0026#39;\u0026#34;,\u0026#34;ttl\u0026#34;:120}\u0026#39; \\\n             | python -c \u0026#34;import sys,json;print(json.load(sys.stdin)[\u0026#39;result\u0026#39;][\u0026#39;id\u0026#39;])\u0026#34;)\n# Save info for cleanup\nif [ ! -d /tmp/CERTBOT_$CERTBOT_DOMAIN ];then\n        mkdir -m 0700 /tmp/CERTBOT_$CERTBOT_DOMAIN\nfi\necho $ZONE_ID \u0026gt; /tmp/CERTBOT_$CERTBOT_DOMAIN/ZONE_ID\necho $RECORD_ID \u0026gt; /tmp/CERTBOT_$CERTBOT_DOMAIN/RECORD_ID\n\n# Sleep to make sure the change has time to propagate over to DNS\nsleep 25\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eto clean dns already generated with \u003ccode\u003ecleanup.sh\u003c/code\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/bash\n\n# Get your API key from https://www.cloudflare.com/a/account/my-account\nAPI_KEY=\u0026#34;api_toke_cloudflare\u0026#34;\nEMAIL=\u0026#34;your_email\u0026#34;\n\nif [ -f /tmp/CERTBOT_$CERTBOT_DOMAIN/ZONE_ID ]; then\n        ZONE_ID=$(cat /tmp/CERTBOT_$CERTBOT_DOMAIN/ZONE_ID)\n        rm -f /tmp/CERTBOT_$CERTBOT_DOMAIN/ZONE_ID\nfi\n\nif [ -f /tmp/CERTBOT_$CERTBOT_DOMAIN/RECORD_ID ]; then\n        RECORD_ID=$(cat /tmp/CERTBOT_$CERTBOT_DOMAIN/RECORD_ID)\n        rm -f /tmp/CERTBOT_$CERTBOT_DOMAIN/RECORD_ID\nfi\n\n# Remove the challenge TXT record from the zone\nif [ -n \u0026#34;${ZONE_ID}\u0026#34; ]; then\n    if [ -n \u0026#34;${RECORD_ID}\u0026#34; ]; then\n        curl -s -X DELETE \u0026#34;https://api.cloudflare.com/client/v4/zones/$ZONE_ID/dns_records/$RECORD_ID\u0026#34; \\\n                -H \u0026#34;X-Auth-Email: $EMAIL\u0026#34; \\\n                -H \u0026#34;X-Auth-Key: $API_KEY\u0026#34; \\\n                -H \u0026#34;Content-Type: application/json\u0026#34;\n    fi\nfi\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eWhen you already done with execute the \u003ca href=\"#the-command-is-particulary-like-this\"\u003ecommand\u003c/a\u003e, you must already have certificates at \u003ccode\u003e/etc/letsencrypt/live/domain_name/\u003c/code\u003e and just add the certificates to your nginx config.\u003c/p\u003e\n\u003cp\u003eExample like this\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eserver {\n....................\n    listen 443 ssl; # managed by Certbot\n    ssl_certificate /etc/letsencrypt/live/your_domain_name/fullchain.pem; # managed by Certbot\n    ssl_certificate_key /etc/letsencrypt/live/your_domain_name/privkey.pem; # managed by Certbot\n    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003etaraaaa\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ereference: \u003ca href=\"https://eff-certbot.readthedocs.io/en/stable/using.html#pre-and-post-validation-hooks\"\u003ehttps://eff-certbot.readthedocs.io/en/stable/using.html#pre-and-post-validation-hooks\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/generate-ssl-dns/","title":"Generate SSL from DNS challanges with certbot"},{"content":"\u003cp\u003e\u003cimg src=\"https://images.unsplash.com/photo-1625258110620-b213f56b9267?ixlib=rb-4.0.3\u0026amp;ixid=MnwxMjA3fDB8MHxzZWFyY2h8MjN8fGVtZXJnZW5jeXxlbnwwfHwwfHw%3D\u0026amp;auto=format\u0026amp;fit=crop\u0026amp;w=400\u0026amp;q=60\" alt=\"Emergency\"\u003e\u003c/p\u003e\n\u003cp\u003eUntuk mengatasi error nginx seperti dibawah ini\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003enginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eJalankan perintah berikut\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003esudo fuser -k 80/tcp\n\nservice nginx start\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSemoga aman servermu :))\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/nginx-error/","title":"nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/marabou-stork-bird-closeup-head-with-natural-background_488145-1161.jpg?w=1380\u0026amp;t=st=1667445952~exp=1667446552~hmac=fad6ae320097c71e87b13dfb720e25a7f3a34c8a2d52de2a5ac1b3ebfd2bee72\" alt=\"gunicorn\"\u003e\u003c/p\u003e\n\u003cp\u003eentah kenapa saya nyari gambar di freepik kebanyakan muncul itu, yaudahlahyaaa.\u003c/p\u003e\n\u003cp\u003eIni adalah cara menjalankan file python menggunakan gunicorn. Pada contoh ini misal kita memilik api server dari file python dengan nama \u003ccode\u003eapi_flask.py\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eMaka cara menjalankannya adalah seperti ini:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003egunicorn --bind 0.0.0.0:5000 api_flask:app -w 2 --log-config logging.ini --access-logfile -\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eketerangan:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eapi_flask = nama file python yg akan dijalankan. nama asli \u003ccode\u003eapi_flask.py\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e-w = workers yang akan dijalankan.\u003c/p\u003e\n\u003cp\u003e-—log-config = file log config yang akan digunakan untuk logging gunicorn.\u003c/p\u003e\n\u003cp\u003econtoh log file gunicorn.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003elogging.ini\u003c/strong\u003e\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e[loggers]\nkeys=root\n\n[handlers]\nkeys=consoleHandler\n\n[formatters]\nkeys=simpleFormatter\n\n[logger_root]\nlevel=DEBUG\nhandlers=consoleHandler\n\n[handler_consoleHandler]\nclass=StreamHandler\nlevel=DEBUG\nformatter=simpleFormatter\nargs=(sys.stdout,)\n\n[formatter_simpleFormatter]\nformat=[%(asctime)s] [%(process)d] [%(levelname)s] - %(module)s - %(message)s\ndatefmt=%Y-%m-%d %H:%M:%S %z\n\u003c/code\u003e\u003c/pre\u003e","description":null,"image":null,"permalink":"http://localhost:1313/posts/cara-run-python-gunicorn/","title":"Cara Menjalankan Python File Menggunakan Gunicorn"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-vector/flat-versus-vs-banner-screen-design_1017-32319.jpg?w=1480\u0026amp;t=st=1667445851~exp=1667446451~hmac=282bbd60a75f0cff58d4115c2243ef57e0805a6054432df853157ac9330c556d\" alt=\"Perbandingan\"\u003e\nNahhh\nMengapa perlu ada catatan ini? ya karena memang saya bingung dan harus mencari tahu sendiri apa berbedaanya. Kan nggak lucu dong hanya karena sering melihat tapi tidak tahu persis apa konsep yang sebenarnya terjadi. wkkkk\u003c/p\u003e\n\u003cp\u003eSebelum mengetahui perbedaan Stateless dan Statefullset pada kubernetes, urutan yang perlu kita pahami adalah mengetahui aplikasi stateless dan statefull terlebih dahulu, dan ini perbedaanya teng tenggg\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAplikasi Stateless\u003c/th\u003e\n          \u003cth\u003eAplikasi Statefull\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDigunakan untuk request baru. Ex: Aplikasi backend dari nodejs\u003c/td\u003e\n          \u003ctd\u003eDigunakan untuk menyimpan data. Ex: mysql\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eTidak menyimpanpan state sebelumnya\u003c/td\u003e\n          \u003ctd\u003eMenyimpan state sebelumnya\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDeploy di kubernetes mengunakan mode deployment\u003c/td\u003e\n          \u003ctd\u003eDeploy di kubernetes menggunakan mode statefullset\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSetelah mengetahui perbadaan basic dari masing-masing aplikasi tersebut, harusnya sudah ada gambaran lah ya.\u003c/p\u003e\n\u003cp\u003ePerbedaan Stateless dan Statefullset pada kubernetes\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eStateless\u003c/th\u003e\n          \u003cth\u003eStatefullSet\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eIdentical and Interchangable\u003c/td\u003e\n          \u003ctd\u003eNot identical\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePod yang dibuat waktunya acak dan menggunakan random hash\u003c/td\u003e\n          \u003ctd\u003eMemiliki fix ordered name\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePod bisa dihapus/dibuat pada waktu yang bersamaan\u003c/td\u003e\n          \u003ctd\u003ePod tidak bisa dihapus/dibuat pada waktu bersamaan\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDeploy menggunakan Deployment\u003c/td\u003e\n          \u003ctd\u003eDeploy menggunakan Statefullset\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePersistent volume tidak bisa digunakan oleh pod lain ketika recreate\u003c/td\u003e\n          \u003ctd\u003ePersistent volume bisa digunakan oleh pod lain ketika recreate\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eCatatan didalam catatan\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estateful kurang cocok untuk containerization\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003estatefull\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ememiliki fix oredered name\u003c/li\u003e\n\u003cli\u003epod tidak akan dibuat ketika previous pod belum selesai\u003c/li\u003e\n\u003cli\u003ecan\u0026rsquo;t be created/deleted at same time\u003c/li\u003e\n\u003cli\u003ecan\u0026rsquo;t be randomly addressed\u003c/li\u003e\n\u003cli\u003ereplica pods are not identical\u003c/li\u003e\n\u003cli\u003epod yang dinyalakan akan tetap sama idnya meskipun baru/sudah dihapus\u003c/li\u003e\n\u003cli\u003eakan ada namanya master slave pada pod, karena jika semua pod dapat RW(Read and Write) maka akan terjadi inconsistency data\u003c/li\u003e\n\u003cli\u003estorage masing\u0026quot; pod akan berbeda\u003c/li\u003e\n\u003cli\u003eakan continously sync\u003c/li\u003e\n\u003cli\u003ejika scale up pod maka pod yang baru akan clone dari previous pod, bukan dari sembarang pod\u003c/li\u003e\n\u003cli\u003ebaiknya ketika menggunakan statefullset dengan persistence volume yang tipenya remote storage, karena sewaktu waktu pod dibuat ulang di node yang berbeda masik bisa mengakses persistent volume\u003c/li\u003e\n\u003cli\u003emasing masing pod memiliki service sendiri\u003c/li\u003e\n\u003cli\u003eselalu akan membuat service headless yang digunakan untuk koneksi antar worker pod untuk data tetap pada state yg uptodate\u003c/li\u003e\n\u003c/ul\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/statefuld-stateless/","title":"Perbedaan Stateless dan Statefull pada Kubernetes"},{"content":"\u003cp\u003e\u003cimg src=\"https://images.unsplash.com/photo-1562654501-a0ccc0fc3fb1?ixlib=rb-4.0.3\u0026amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8\u0026amp;auto=format\u0026amp;fit=crop\u0026amp;w=1632\u0026amp;q=80\" alt=\"Certmanager\"\u003e\nNahhh\u003c/p\u003e\n\u003cp\u003eKali ini catatan yang menurut saya agak banyak langkah, tanpa basa basi please welcome \u003ccode\u003eMembuat Domain pada Kubernetes menjadi HTTPS menggunakan cert-manager\u003c/code\u003e. Ada 2 bagian yang perlu diperhatikan\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eingress\u003c/li\u003e\n\u003cli\u003ecert-manager\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"ingress\"\u003eIngress\u003c/h2\u003e\n\u003cp\u003ePertama kita perlu menginstall ingress pada cluster k8s terlebih dahulu, untuk metode installnya bisa menggunakan helm atau jika kita menggunakan digitalocean bisa menggunakan fitur Install Kubernetes 1-Click Apps.\nInstall ingress melalui helm:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehelm upgrade --install ingress-nginx ingress-nginx \\\n  --repo https://kubernetes.github.io/ingress-nginx \\\n  --namespace ingress-nginx --create-namespace\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"cert-manager\"\u003eCert-Manager\u003c/h2\u003e\n\u003cp\u003eSama halnya dengan Ingress jika kita menggunakan DO bisa menginstall melalui Install Kubernetes 1-Click Apps. Jika menginstall melalui helm berikut perintahnya\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.9.1/cert-manager.crds.yaml\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.9.1 \\\n  # --set installCRDs=true\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOke, setelah ingress dan cert-manager telah terinstall. Coba untuk membuat ingress tanpa https sesuai dengan service yang sudah terdeploy.\nContoh ingress yaml:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: RELEASE-NAME-demo\n  labels:\n    helm.sh/chart: demo-0.1.0\n    app.kubernetes.io/name: demo\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/version: \u0026#34;0.28.0\u0026#34;\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n    - host: \u0026#34;k8s-backend.demo.id\u0026#34;\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: RELEASE-NAME-demo\n                port:\n                  number: 80\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePastikan sudah bisa mengakses domain yang sudah dibuat melalui ingress, contoh hasil ingress sudah terbuat semacam ini:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eNAMESPACE   NAME          CLASS    HOSTS                   ADDRESS        PORTS     AGE\ndefault     demo          nginx    k8s-backend.demo.id    167.x.x.x       80        21h\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"issuer\"\u003eIssuer\u003c/h2\u003e\n\u003cp\u003eBerfungsi untuk request dan genarate TLS yang valid untuk ingress yang sedang digunakan, dalam hal ini yaitu ingress-nginx. Berikut contoh file yaml issuer untuk staging:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n name: letsencrypt-staging\n namespace: cert-manager\nspec:\n acme:\n   # The ACME server URL\n   server: https://acme-staging-v02.api.letsencrypt.org/directory\n   # Email address used for ACME registration\n   email: yudi@mail.com\n   # Name of a secret used to store the ACME account private key\n   privateKeySecretRef:\n     name: letsencrypt-staging\n   # Enable the HTTP-01 challenge provider\n   solvers:\n   - http01:\n       ingress:\n         class:  nginx\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eApply file yaml tersebut, untuk mengecek apakah sudah terpasang bisa menggunakan perintah \u003ccode\u003ekubectl get clusterissuers.cert-manager.io\u003c/code\u003e. Buat juga file yaml untuk production\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: cert-manager\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: yudi@mail.com\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    # Enable the HTTP-01 challenge provider\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSetelah kedua file yaml tersebut terpasang, tambahkan pada file ingress yang tadi sudah dibuat sehingga menjadi seperti ini\ningress-https.yaml\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: RELEASE-NAME-demo\n  labels:\n    helm.sh/chart: demo-0.1.0\n    app.kubernetes.io/name: demo\n    app.kubernetes.io/instance: RELEASE-NAME\n    app.kubernetes.io/version: \u0026#34;0.28.0\u0026#34;\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt-staging\nspec:\n  tls:\n    - hosts:\n        - \u0026#34;k8s-backend.demo.id\u0026#34;\n      secretName: demo-tls\n  rules:\n    - host: \u0026#34;k8s-backend.demo.id\u0026#34;\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: RELEASE-NAME-demo\n                port:\n                  number: 80\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAgar generate ssl aman dan untuk menghindari limit request kita menggunakan cluster-issuer staging terlebih dahulu. Jika sudah dirasa aman dan log pada pod cert-manager tidak ada masalah, baru pindah ke letsencrypt yang prod.\u003c/p\u003e\n\u003cp\u003eJika tidak nampak error harusnya sudah bisa berjalan dan https sudah terinstall pada domain kita. yuhuuuu\u003c/p\u003e\n\u003cp\u003esumber dan referensi:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://marketplace.digitalocean.com/apps/cert-manager\"\u003ehttps://marketplace.digitalocean.com/apps/cert-manager\u003c/a\u003e\n\u003ca href=\"https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes\"\u003ehttps://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-with-cert-manager-on-digitalocean-kubernetes\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/membuat-https-dengan-cert-manager/","title":"Membuat Domain pada Kubernetes menjadi HTTPS menggunakan cert-manager"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-vector/people-house-entrance-enter-exit-home-vector-flat-illustration-happy-characters-open-door-lock-with-key-taking-out-trash-leaving-apartment-come-back-from-shopping_107791-11723.jpg?w=1380\u0026amp;t=st=1667445669~exp=1667446269~hmac=1745cb897be214793bc9c968706867ecc6ca79217d445e1d1e37b2bb9fa4d6e1\" alt=\"RBAC\"\u003e\nSaya berharap untuk yang membaca ini berarti sedikit banyak sudah mengetahui tentang kubernetes (k8s), apabila belum tahu yaaa nggak papa juga sih. :D\u003c/p\u003e\n\u003cp\u003eUtuk kali ini yang ingin saya tulis tentang RBAC adalah:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecara membuat user untuk mengakses kubernetes\u003c/li\u003e\n\u003cli\u003emembuat role untuk user\u003c/li\u003e\n\u003cli\u003emembuat rolebinding untuk user\u003c/li\u003e\n\u003cli\u003emembatasi user untuk mengakses namespace tertentu\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"pertama--membuat-user-kubernetes\"\u003ePertama | Membuat user kubernetes\u003c/h2\u003e\n\u003cp\u003ePada dasarnya ketika kita setup sebuah cluster kubernetes ada beberapa cara untuk mengakses cluster menggunakan kubectl, salah satunya menggunakan certificates/ssl. Nah untuk kali ini kita membuat user yang menggunakan certificate. Langkah pertama adalah generate rsa, supaya menggampangkan gambaran untuk kali ini nama usernya adalah \u003ccode\u003eyudi\u003c/code\u003e jadi sesuaikan jika akan diubah.\u003c/p\u003e\n\u003cp\u003eBuat private key untuk user yudi (yudi.key)\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eopenssl genrsa -out yudi.key 2048\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eBuat certificate signing request (yudi.csr)\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eopenssl req -new -key yudi.key -out yudi.csr -subj \u0026#34;/CN=$i/O=finance\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ca href=\"https://gist.github.com/yuswitayudi/98eaeb2b9a6077868bc96159a2d998f6\"\u003ehttps://gist.github.com/yuswitayudi/98eaeb2b9a6077868bc96159a2d998f6\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/rbac-untuk-user/","title":"Membuat RBAC(Role Based Access Control) user pada kubernetes"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-photo/vintage-clock_74190-1472.jpg?w=1380\u0026amp;t=st=1667445622~exp=1667446222~hmac=f3b9a7a6ccf03d5d05613b3f08a9c42028833518cb6a756e0c753854fa7cb84f\" alt=\"Zero Downtime\"\u003e\nOkee, untuk kali ini catatan yang akan ditulis adalah mengenai hal-hal apa yang perlu diperhatikan ketika menerapkan Zero Downtime pada kubernetes.\u003c/p\u003e\n\u003ch2 id=\"pertama--replicasets\"\u003ePertama | ReplicaSets\u003c/h2\u003e\n\u003cp\u003eMenentukan ReplicaSets pada jumlah batas minimum, yaitu 3. Kenapa demikian? Ini dikarenakan jika suatu saat ada pods kita yang crash kita masih memiliki pods yang lain yang masih berjalan.\u003c/p\u003e\n\u003cp\u003eLinks terkait \u003ca href=\"https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\"\u003eReplicaSets\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"kedua--podantiaffinity\"\u003eKedua | podAntiAffinity\u003c/h2\u003e\n\u003cp\u003eBayangkan apabila anda memiliki 3 replicasets, akan tetapi semuanya berada pada 1 node dan node itu tidak bisa diakses. Maka semua service tidak bisa diakses juga, maka butuh yang namanya \u0026ldquo;podAntiAffinity\u0026rdquo; yang berguna untuk melakukan pemasangan pod sesuai dengan rule yang sudah kita tentukan.\u003c/p\u003e\n\u003cp\u003ePada kasus ini jika kita memiliki 3 replicasets, maka hal yang terbaik yang bisa kita lakukan adalah memisah replica tersebut ke setiap node yang kita miliki, sehingga tidak menumpuk di 1 node saja. Apa bila sewaktu-waktu ada node yang tidak bisa diakses, masih ada replicasets kita yang berjalan pada node lain.\u003c/p\u003e\n\u003cp\u003eLinks terkait \u003ca href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\"\u003epodAntiAffinity\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003econtoh yaml\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eapiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-cache\nspec:\n  selector:\n    matchLabels:\n      app: store\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: store\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - store\n            topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34;\n      containers:\n      - name: redis-server\n        image: redis:3.2-alpine\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"ketiga--resource-declaration\"\u003eKetiga | Resource Declaration\u003c/h2\u003e\n\u003cp\u003eIni perlu untuk ditentukan untuk menanggulangi ketika node mengalami overused, dikarenakan dari deployment yang sudah terpasang menggunakan default resource, padahal untuk penggunaan tidak terlalu tinggi.\u003c/p\u003e\n\u003cp\u003eMaka perlu adanya deklarasi untuk minimum dan maksimum resources, agar setiap pod yang jalan menggunakan resource sesuai kebutuhan saja.\u003c/p\u003e\n\u003cp\u003eLink terkait \u003ca href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\"\u003eResource Declaration\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"keempat--readiness--liveness-probe\"\u003eKeempat | Readiness \u0026amp; Liveness Probe\u003c/h2\u003e\n\u003cp\u003eDalam menjalankan pod kita butuh untuk mengetahui pod tersebut sudah bisa diakses atau belum, lebih dari itu meskipun pod sudah bisa diakses namun pada kondisi tertentu pod gagal untuk memproses service padahal secara status masih running.\nOleh karena itu kita membutuhkan yang namanya Readiness \u0026amp; Liveness, yang berfungsi:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emengetahui pod sudah siap untuk diakses atau belum\u003c/li\u003e\n\u003cli\u003emengetahui jika pod tidak bisa diakses pada waktu tertentu ketika pod sudah berjalan\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003edari kegunaan tersebut pod yang tidak bisa diakses akan otomatis recreate(dibuat ulang) sesuai dengan parameter yang kita tentukan.\u003c/p\u003e\n\u003cp\u003eLink terkait \u003ca href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\"\u003eReadiness \u0026amp; Liveness\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"kelima--rolling-update-optionalimprovement\"\u003eKelima | Rolling Update (Optional\u0026amp;improvement)\u003c/h2\u003e\n\u003cp\u003ePada dasarnya untuk metode deployment rolling update sudah ada secara default. Namun sebelum itu kita perlu paham terlebih dahulu apa itu maxSurge dan maxUnavailable. maxSurge berfungsi untuk mengatur berapa banyak pod yang akan kita tambahkan ketika terjadi update. maxUnavailable berfungsi untuk mengatur berapa banyak pod yang akan dihilangkan ketika proses rolling update.\u003c/p\u003e\n\u003cp\u003eMasing-masing antara maxSurge dan maxUnavailable bisa diatur menggunakan persentase maupun jumlah pod.\u003c/p\u003e\n\u003cp\u003eLink terkait \u003ca href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment\"\u003eRolling Update (maxSurge \u0026amp; max unavailable)\u003c/a\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/zerodowntime-deployment-k8s/","title":"Hal-hal yang perlu diperhatikan untuk menerapkan Zero Downtime pada k8s"},{"content":"\u003cp\u003e\u003cimg src=\"https://images.unsplash.com/photo-1486312338219-ce68d2c6f44d?ixlib=rb-4.0.3\u0026amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8\u0026amp;auto=format\u0026amp;fit=crop\u0026amp;w=1472\u0026amp;q=80\" alt=\"alt text\" title=\"Title\"\u003e\u003c/p\u003e\n\u003ch2 id=\"ini-adalah-langkah-posting-menggunakan-hugo\"\u003eIni adalah langkah Posting menggunakan hugo\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBuat file markdown baru\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003ehugo new posts/nama_file.md\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eEdit file postingan sesuai kesukaanmu\u003c/li\u003e\n\u003cli\u003eBuat file \u003ccode\u003edeploy.sh\u003c/code\u003e untuk otomatisasi push code dan deploy pada github.io\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#!/bin/bash\n\necho -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34;\n\n# Generate file statis\nhugo # if using a theme, replace by `hugo -t \u0026lt;yourtheme\u0026gt;`\n\n# pindah ke direktoru publik\ncd public\n# tambahkan perubahan ke Git\ngit add -A\n\n# Buat sebuah commit baru\nmsg=\u0026#34;rebuilding site `date`\u0026#34;\nif [ $# -eq 1 ]\n  then msg=\u0026#34;$1\u0026#34;\nfi\ngit commit -m \u0026#34;$msg\u0026#34;\n\n# Push atau puload ke Github\ngit push origin master\n\n# Balik ke direktori sebelumnya\ncd ..\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSesuaikan branch jika anda menggunakan branch selain \u003ccode\u003emaster\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"dan-taraaa\"\u003edan taraaa~~\u003c/h2\u003e\n\u003cp\u003eHarusnya code sudah ter-push ke github dan sedang proses build static di circle-ci atau tempat Continous Deployment favoritmu. Untuk cara koneksi github ke circle CI akan kita bahas di postingan selanjutnya, kalo saya mau cuwauuu.\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/cara-posting/","title":"Cara posting menggunakan hugo"},{"content":"\u003cp\u003e\u003cimg src=\"https://img.freepik.com/free-vector/realistic-single-silver-microphone-retro-design-with-black-switch_1284-33542.jpg?w=900\u0026amp;t=st=1667445579~exp=1667446179~hmac=19355961a0a9fd3a9b3cf81de2ad42cd5ac3509250cf252e637361a3d88413dc\" alt=\"Ini Intro\"\u003e\nSebenernya ini reset halaman sih.\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/intro/","title":"Intro"}]